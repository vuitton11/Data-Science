{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Student Information</h3> Please provide information about yourself.<br>\n",
    "<b>Name</b>: Veton Abazovic<br>\n",
    "<b>NetID</b>:va187<br>\n",
    "<b>Recitation (01/02)</b>: 02<br>\n",
    "<b>Notes to Grader</b> (optional):<br>\n",
    "<br><br>\n",
    "<b>IMPORTANT</b>\n",
    "Your work will not be graded withour your initials below<br>\n",
    "I certify that this lab represents my own work and I have read the RU academic intergrity policies at<br>\n",
    "<a href=\"https://www.cs.rutgers.edu/academic-integrity/introduction\">https://www.cs.rutgers.edu/academic-integrity/introduction </a><br>\n",
    "<b>Initials</b>:    VA \n",
    "\n",
    "\n",
    "<h3>Grader Notes</h3>\n",
    "<b>Your Grade<b>:<br>\n",
    "<b>Grader Initials</b>:<br>\n",
    "<b>Grader Comments</b> (optional):<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Deep Learning with TensorFlow\n",
    "\n",
    "** This final project is due Thursday May 10, 2019 at 11:59pm (graded on accuracy and completeness) **\n",
    "\n",
    "In this project we will work through the process of:\n",
    "1. installing TensorFlow in your local environment\n",
    "2. Learn some basics of how to use TensorFlow API\n",
    "3. Implement a classical linear regression model with TensorFlow\n",
    "4. Implement a k-means for clustering images\n",
    "5. Import and Train a model with FashionNIST dataset \n",
    "6. Predict new images based on the trained model\n",
    "\n",
    "This project should give you some basic knowledge on how to work with deep learning systems. Although we do not implement any neural networks from scratch in this lab, it is also important for you to have an understanding of how neural networks work.Please refer to classnotes to see how neural networks are trained. In this project we will use existing libraries, but by selecting and tuning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0 - Install TensorFlow\n",
    "This is our first experience with deep learning methods. We need to set things up first. Execute the lines below to get the setup going. Set up only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0.1. tf requires a numpy update\n",
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0.2. install tensorflow\n",
    "!pip install --user tensorflow \n",
    "# you can also specify what version of TF you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0.3. install keras API\n",
    "!pip install --user keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Task 0.4. all the import we need\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras     # we can also access keras API with tf.keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "# now we are ready to move on to doing some amazing things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Getting to know TensorFlow (TF)\n",
    "TensorFlow is a framework. This core open source library help you develop and train ML models. You can install tensorflow locally or run tensorflow on colab.research.google.com directly in your browser. TensorFlow supports wide range of devices and API's, including javascript for deploying mobile ML based applications. It can run on single CPU systems, GPUs, TPU's as well as mobile devices and large scale distributed systems of hundreds of machines.\n",
    "TensorFlow is its own framework. Everything in TF is implemented as a computational graph. The graph nodes are executed at later times. \n",
    "Let us understand some of its basic API functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1 Learn the basics\n",
    "TensorFlow is a framework to allow efficient computing with multi-dimensional data. All the calls to TF are executed through API calls. It uses python (altough there are other TF suported languages now such as R)  in the backend to exceute these commands. All objects are stored as tensors, a form of a vector. A TF graph can be evaluated using a tensorflow session. Here are few examples. Just run the cells and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two constant tensors\n",
    "two_node = tf.constant(8)     \n",
    "three_node = tf.constant(9)\n",
    "# sum of the two tensors\n",
    "sum_node = two_node + three_node    # need a session to evaluate the value of sum_node\n",
    "# product the two objects\n",
    "pdt_node = two_node * three_node\n",
    "print(sum_node)                   # should notice that they are just unevaluated tensors\n",
    "# A session is an object instance that encapsulate the state and operations on the object.\n",
    "with tf.Session() as sess:\n",
    "    # call run method in the session object to evaluate the two_node and sum_node objects\n",
    "    print(sess.run([sum_node, pdt_node]))\n",
    "    \n",
    "# QUESTION : Explain why print(sum_node) does not show the value 17\n",
    "#  It is because its a tensor object instead of a value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops in TensorFlow\n",
    " read more about loops in : https://www.tensorflow.org/api_docs/python/tf/while_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is some sample code on how to write loops. Just modify the code\n",
    "i = tf.constant(0)\n",
    "cond = lambda i: tf.less(i, 12)\n",
    "body = lambda i: tf.add(i, 1)\n",
    "r = tf.while_loop(cond, body, [i])   # returns the final value of i\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(r))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example of a While Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some other demo code on how to use tf.while\n",
    "def cond(t1, t2):\n",
    "    return tf.less(t1, t2)\n",
    "\n",
    "def body(t1, t2):\n",
    "    return [tf.add(t1, 1), t2]\n",
    "\n",
    "t1 = tf.constant(2)\n",
    "t2 = tf.constant(4)\n",
    "\n",
    "res = tf.while_loop(cond, body, [t1, t2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(res))\n",
    "\n",
    "# question: What happens if we change both t1 and t2 at the same time?\n",
    "#If they are swapped then the addition does not occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2 Find the first 10 Fibbonachchi numbers\n",
    "In this activity we will write TF code using tensorflow framework to output the first 10 Fibonnachi numbers 1 1 2 3 5 8 13 21 34 55 as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN SOLUTION\n",
    "# define the first 2 terms of the sequence\n",
    "a = tf.Variable(0)\n",
    "b = tf.Variable(1)\n",
    "\n",
    "newterm = tf.add(a,b)     \n",
    "\n",
    "with tf.control_dependencies([newterm]):\n",
    "    update1 = tf.assign(a, b)\n",
    "\n",
    "    with tf.control_dependencies([update1]):\n",
    "        update2 = tf.assign(b, newterm)\n",
    "#initializes\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# run\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    fibonacci = [a.eval(), b.eval()]\n",
    "    for i in range(9):\n",
    "         next, up1, up2 = sess.run([newterm, update1, update2])\n",
    "         fibonacci.append(next)\n",
    "    \n",
    "    #print(type(fibonacci))\n",
    "    fibonacci.pop(0)\n",
    "    print(fibonacci)\n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3 Activating Placeholders\n",
    "In the execution of the tensorflow objects, it is possible that you will have to wait for some results to show after a cycle of operations. A placeholder is more like a variable that you would declare in a program and initialize later. In general, Insert a placeholder for a tensor that will be always fed.We can use tf.placeholder to make calls to placeholder tensors. Read more about placeholders at https://www.tensorflow.org/api_docs/python/tf/placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN SOLUTION\n",
    "\n",
    "# two tensor placeholders that can hold int32 data\n",
    "x = tf.placeholder(tf.int32, name='x')       # x can hold int32 \n",
    "y = tf.placeholder(tf.int32,name='y')       # y can hold int32 \n",
    "\n",
    "# Find sum, difference and product\n",
    "sumOf = tf.add(x,y)            # use tf.sum\n",
    "diff =  tf.subtract(x,y)            # use tf.subtract\n",
    "prod =  tf.multiply(x,y)            # use tf.multiply\n",
    "\n",
    "# store inputs as a dictionary\n",
    "dict_values = {x:20,y:30}          # store values x = 20 and y = 30 as a dictionay\n",
    "\n",
    "## END SOLUTION\n",
    "\n",
    "# run a session\n",
    "with tf.Session() as sess:\n",
    "    print('Sum',sess.run(sumOf,feed_dict=dict_values))\n",
    "    print('Difference',sess.run(diff,feed_dict=dict_values))\n",
    "    print('Product',sess.run(prod,feed_dict=dict_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.4 Matrix Operations\n",
    "The power of the concept of a tensor is its ability to treat all data as vectors. This makes it easy to parallelize many operations such as matrix multiplication. If you run code on colab.research.google.com, you can also choose some server configuration (GPU, TPU) to run your code faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN SOLUTION\n",
    "\n",
    "import numpy as np\n",
    "# initialize numpy arrays a and b\n",
    "a = np.array([[5.0,5.0]])               \n",
    "b = np.array([[2.0],[2.0]])  \n",
    "\n",
    "# define two tensors mat1 and mat2 using numpy arrays a and b\n",
    "mat1 = tf.convert_to_tensor(a)                # mat1 is a rank-1 tensor               \n",
    "mat2 = tf.convert_to_tensor(b)                # mat2 is a rank-2 tensor  \n",
    "\n",
    "# build a matrix multiplier tensor\n",
    "matrix_pdt = tf.matmul(mat1, mat2)    # use tf.matmul to find the result\n",
    "# run a session and perform the operartions\n",
    "\n",
    "## END SOLUTION\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(matrix_pdt)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read API Docs\n",
    "The above examples were just few things to demonstrate that tensorflow is a complete framework and one can code an entire program using tensor objects. Read more at https://www.tensorflow.org/api_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Handwriting classification with k-means\n",
    "In this part, we will use the MNIST dataset to import 60,000 handwritten digits and implement k-means to classify them. MNIST is a benchmark dataset that can help learn many things about Deep Learning basics.\n",
    "This Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. You can import this data from keras.datasets or from tensorflow.examples.tutorials.mnist. You can use either form in your solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import from kears. The data will be downloaded as numpy.ndarray\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# plot images to visualize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt1 = plt.imshow(x_train[0].reshape(28,28))   # visualize image[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1 import data from MNIST\n",
    "In this task we will import the MNIST digits dataset and learn the properties of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# get the MNIST Dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "type(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 Find Stats\n",
    "Find how many training and validation images are in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# how many train images are in the dataset?\n",
    "train_images = len(mnist.train.labels)           \n",
    "print(\"train images: \", train_images)\n",
    "# how many validation images are in the dataset?\n",
    "validation_images = len(mnist.validation.labels) \n",
    "print(\"validation images: \" , validation_images)\n",
    "# END SOLTUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3 Visualize the image\n",
    "Each image is a gray-scale 28x28 (784 pixles) image. You can treat the image as a rank-1 tensor (or 1D vector) of size 784 with each value between 0-255. We can plot some images here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt1 = plt.imshow(mnist.train.images[2].reshape(28,28))   # visualize image[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4 Implementing k-means algorithm\n",
    "In this part we will implement k-means algorithm to identify possible clusters in this data. An image in this data set is a gray-scale 28 x 28 image (784 pixels with values from 0-255). We will use the eucledean distance between two images to see the \"distance\" between them. Since we are working in high dimension (784) we can only understand the concept of distance here. We will use the k-means algorithm that was discussed in lecture 12.2 slide 15. You do not need to use TF modules t do this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4.1 Implement the k-means algorithm (vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorized version of k-means is given on the slide 15 of Lecture 12.2.\n",
    "# Just use the same code.\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "def kmeans(X, k, max_iter=10, rand_seed=0):\n",
    "    np.random.seed(rand_seed)\n",
    "    Mu = X[np.random.choice(X.shape[0],k),:]\n",
    "    for i in range(max_iter):\n",
    "        D = -2*X@Mu.T + (X**2).sum(axis=1)[:,None] + (Mu**2).sum(axis=1)\n",
    "        y = np.argmin(D,axis=1)\n",
    "        Mu = np.array([np.mean(X[y==i],axis=0) for i in range(k)])\n",
    "    loss = np.linalg.norm(X - Mu[np.argmin(D,axis=1),:])**2/X.shape[0]\n",
    "    return Mu, y, loss\n",
    "\n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4.2 Cluster MNIST images.\n",
    "Experiment to Find a good value of k such that most centers look like images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN SOLUTION\n",
    "\n",
    "# start with k = 50\n",
    "k = 50\n",
    "# flatten each image to be 784 value vector and form 55000 x 784 matrix X\n",
    "X = mnist.train.images.reshape(55000, 784)\n",
    "\n",
    "# Choose k random centers from X. That is k random data points become the original cluster centers\n",
    "Mu = X[np.random.choice(X.shape[0], k), ::]\n",
    "# call the kmeans algorithm defined above. Be sure to have the proper arguments passed to kmeans.\n",
    "[Mu, y, loss] = kmeans(X, k)\n",
    "\n",
    "# Plot the cluster centers as images\n",
    "\n",
    "fig, ax = plt.subplots(10, 5, figsize=(10, 10))\n",
    "\n",
    "centers = Mu.reshape(50, 28, 28)\n",
    "#loop\n",
    "for axi, center in zip(ax.flat, centers):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)\n",
    "    \n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.5 (optional for Extra Credit)\n",
    "Implement the k-means++ algorithm to see if better clusters can be obtained in fewer iterations. This task is optional and you can add cells below to perform the operations. Do this part after completing the all the parts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN SOLUTION (OPTIONAL)\n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Linear Regression with TensorFlow\n",
    "We have learned how to implement linear regression using Pandas. In this exercise, we will learn how to use tensorflow to implement linear regression. As stated TensorFlow is a framework that allows you to implement many standard ML algorithms. We will implement Linear Regression using TF libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1 set up\n",
    "Let us set up basic things needed to get tensorflow going. You are asked to experiment with different values of alpha (learning rate), epochs (how many rounds in training. Too little is under-fitting, too much is over-fitting). You need to display the process every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "rng = numpy.random\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# set parameters\n",
    "alpha = 0.01    # alpha is the learning rate\n",
    "epochs = 1000   # One epoch is when an entire dataset is passed both forward and backward through the neural network \n",
    "                # only once\n",
    "step = 50       # every 50 iterations display the results.\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 Read data\n",
    "We will work with our CS 205 Data set. First we create a data frame, then extract midterm and final exam scores as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Training Data\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/midterm_finals_CS205F18.csv\")\n",
    "\n",
    "# use first 110 records for the training data\n",
    "train_X = df.iloc[0:110,[0]].values\n",
    "train_Y = df.iloc[0:110,[1]].values\n",
    "\n",
    "# use the remaining for validation\n",
    "validate_X = df.iloc[110:,[0]].values\n",
    "validate_Y = df.iloc[110:,[1]].values\n",
    "\n",
    "n_samples = train_X.shape[0]\n",
    "print(\"training set size: \", n_samples) \n",
    "print(\"validation set size: \", validate_X.shape[0])    \n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3 Intialize model\n",
    "Set model weights and bias. You may want to try random weights and random bias. If they do not provide a good regression line, try to pre-set some values for weight and bias. The values must be floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input. We will feed the values later using feed_dict.\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "# Variables are manipulated via the tf.Variable class. A tf.Variable represents a tensor \n",
    "# whose value can be changed by running ops on it. tf.Variable exists outside the context of a single session.run call.\n",
    "\n",
    "theta = tf.Variable(1.0, tf.float32)\n",
    "b = tf.Variable(1.0, tf.float32)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4 Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a linear model using tf.add and tf.multiply. The model provides y = theta*x + bias\n",
    "# BEGIN SOLUTION\n",
    "predictor = tf.add(tf.multiply(X, theta), b)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average squared error using TF calls\n",
    "# BEGIN SOLUTION\n",
    "cost = tf.reduce_sum(tf.pow(predictor-Y, 2)) / (2 * n_samples)\n",
    "# Use the Gradient descent Optimizer from tf.train\n",
    "optimizer = tf.train.GradientDescentOptimizer(alpha).minimize(cost)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "## BEGIN SOLUTION\n",
    "init = tf.global_variables_initializer()\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.5 Train the Model\n",
    "Train the linear regression model by running the training data through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess: \n",
    "      \n",
    "    # Initializing the Variables \n",
    "    sess.run(init) \n",
    "      \n",
    "    # Iterating through all the epochs \n",
    "    for epoch in range(epochs): \n",
    "          \n",
    "        # Feeding each data point into the optimizer using Feed Dictionary \n",
    "        for (_x, _y) in zip(train_X, train_Y): \n",
    "            sess.run(optimizer, feed_dict = {X : _x, Y : _y}) \n",
    "          \n",
    "        # Displaying the result after every 50 epochs \n",
    "        if (epoch + 1) % 50 == 0: \n",
    "            # Calculating the cost a every epoch \n",
    "            c = sess.run(cost, feed_dict = {X : train_X, Y : train_Y}) \n",
    "            print(\"Epoch\", (epoch + 1), \": cost =\", c, \"theta =\", sess.run(theta), \"b =\", sess.run(b)) \n",
    "      \n",
    "    # Storing necessary values to be used outside the Session \n",
    "    training_cost = sess.run(cost, feed_dict ={X: train_X, Y: train_Y}) \n",
    "    Weight = sess.run(theta) \n",
    "    bias = sess.run(b) \n",
    "    \n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.6 Display Regression Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphic display\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(theta) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.7 Compare with Validation Data\n",
    "We have some validation samples and let us see where the line fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## BEGIIN SOLUTION\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    plt.plot(validate_X, validate_Y, 'ro', label = 'Original Data')\n",
    "    plt.plot(validate_X, sess.run(theta) * validate_X + sess.run(b), label = 'Fitted Line')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - First Exercise in Deep Learning\n",
    "Deep Learning has become quite popular in recent times (since 2012) due to its remarkable advances in applications including self driving cars, image recognition, and voice to text conversion. The hello world of deep learning (DL) is long considered to be the exercise that uses of the MNIST hand-written digits dataset that was created by Yann Lecun in 1998. Yann LeCunn who is a professor at NYU and Director of AI at Facebook, just won the 2019 Turing Award for his contributions to Deep Learning. Let us go through a series of exercises to learn how to read, train and validate an image data set. In this case the 60,000 handwritten images of 0-9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1 download the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# get the MNIST Dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2 Learn about the data set.\n",
    "Just write some code to answer the following questions. \n",
    "\n",
    "- how big is the training set?\n",
    "- how big is the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# read the MNIST dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# get training images from MNIST\n",
    "train_images = mnist.train.images\n",
    "# how big is test images?\n",
    "test_images = mnist.test.images.shape[0]\n",
    "# how big is validation set?\n",
    "validation_images =mnist.validation.images.shape[0]\n",
    "### END SOLUTION\n",
    "print(\"\\ntest images:\", test_images, \"validation images:\", validation_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3 Visualize the data set\n",
    "Each image is a 28 x 28 gray-scale 2D image. Display the images images[0] and images[1] as 2D images.\n",
    "Then flatten and display image[0] to understand the form of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt1 = plt.imshow(mnist.train.images[0].reshape(28,28))\n",
    "plt2 = plt.imshow(mnist.train.images[1].reshape(28,28),cmap='gist_gray')  # gray scale\n",
    "# flatten images[1] and display\n",
    "plt3 = plt.imshow(mnist.train.images[1].reshape(784,1),cmap='gist_gray',aspect=0.02)\n",
    "\n",
    "# show first 4 images side by side. \n",
    "f, axarr = plt.subplots(2,2)\n",
    "axarr[0,0].imshow(mnist.train.images[0].reshape(28,28))\n",
    "axarr[0,1].imshow(mnist.train.images[1].reshape(28,28))\n",
    "axarr[1,0].imshow(mnist.train.images[2].reshape(28,28))\n",
    "axarr[1,1].imshow(mnist.train.images[3].reshape(28,28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4 Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "## BEGIN SOLUTION\n",
    "\n",
    "# create a placeholder\n",
    "x = tf.placeholder(tf.float32,shape=[None,784])  # 784 = mnist.train.images[1].shape\n",
    "\n",
    "# there are 10 possible numbers that the classifier must recognize\n",
    "# start with a matrix 784x10 of all zeros\n",
    "thetas = tf.Variable(tf.zeros([784,10]))\n",
    "\n",
    "# create a tf.Variable of 10 zeros (we need to find 10 categories 0-9)\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Create the Graph thetas*x + b\n",
    "y = tf.matmul(x, thetas)+b\n",
    "\n",
    "# loss and optimizer\n",
    "y_true = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "# Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a \n",
    "# probability value between 0 and 1.\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=y))\n",
    "\n",
    "# optimizer is a Gradient Descent Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "\n",
    "# training model using optimizer.minimize. Pass cross_entrophy as the argument\n",
    "train = optimizer.minimize(cross_entropy)\n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.5 Create a Session and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize all global variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 1000, 2000,.., 10000 steps on the training set\n",
    "# Using built in batch feeder from mnist for convenience\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for r in range(1000,11000,1000):\n",
    "        for step in range(r):\n",
    "            batch_x , batch_y = mnist.train.next_batch(100)\n",
    "            sess.run(train,feed_dict={x:batch_x,y_true:batch_y})\n",
    "\n",
    "        matches = tf.equal(tf.argmax(y,1),tf.argmax(y_true,1))\n",
    "        acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "        print(\"trained in steps\", r, sess.run(acc,feed_dict={x:mnist.test.images,y_true:mnist.test.labels}))\n",
    "   \n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.6 (challenge task - optional)\n",
    "What is the maximum accuracy you observed in the previous part?  The best models can get closer to 99% accuracy. If you'd like,  you can try to improve the accuracy of the classifier. Please try this after completing all the required activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code and documentation for challenge task (optional). You can add any additional cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Classification of Fashion images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1 Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the fashionMNISt data set and separate training and test sets\n",
    "# use keras.datasets.fashion_mnist\n",
    "from tensorflow import keras\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2 Explore this data set\n",
    "Inspect the dataset. The class names are not included with the data. We will add them.\n",
    "'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add classnames label images (ints) corresponds to thses labels. For example label 0 is T-shirt/top\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "# how many train images and their dimensions\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "train_images.shape, test_images.shape, test_labels.shape, train_labels.shape\n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.3 Visualize images in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize images in this data set\n",
    "## BEGIN SOLUTION\n",
    "img_number = 10000\n",
    "plt.figure()\n",
    "plt.imshow(train_images[img_number])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "print(class_names[train_labels[img_number]])\n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.4 Plotting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first 25 images and their class_names in a 5x5 grid \n",
    "## BEGIN SOLUTION\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.5 Model Creation and Compilation\n",
    "Read about keras model creation at https://keras.io/models/about-keras-models/\n",
    "also read about sequential models at https://keras.io/models/sequential/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a keras.sequential model\n",
    "# use tf.nn.relu as activation. Read more at https://www.tensorflow.org/api_docs/python/tf/nn\n",
    "# use one input layer (784), one hidden layer(128), and one output layer(10) in the model.\n",
    "# we are only going to use one hidden layer in the model\n",
    "# see some examples here: https://keras.io/models/model/\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "model = keras.Sequential([keras.layers.Flatten(input_shape= (28,28)), keras.layers.Dense(128, activation = tf.nn.relu), keras.layers.Dense(10, activation = tf.nn.softmax)])\n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model. See https://keras.io/models/model/.\n",
    "# you can use the adam optimizer. \n",
    "# The choice of optimization algorithm for your deep learning model is really important\n",
    "# The Adam optimization algorithm is an extension to stochastic gradient descent \n",
    "# that is better for deep learning applications in computer vision.\n",
    "## BEGIN SOLUTION\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.6 Fit the model\n",
    "Fit the model to training images. https://keras.rstudio.com/reference/fit.html\n",
    "You can use any number of epochs (eg. 10). You should see the accuracy increasing and loss decreasing for training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN SOLUTION\n",
    "model.fit(train_images, train_labels, epochs = 5)\n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.7 Test Accuracy\n",
    "Using model.evaluate (see https://keras.io/models/model/) find the test_loss and test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN SOLUTION\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Testing Accuracy... ', test_acc)\n",
    "print('Testing Loss... ', test_loss)\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.8 Predicting\n",
    "using model.predict, create a prediction model for test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN SOLUTION\n",
    "# create a prediction model for the test images\n",
    "predictions = model.predict(test_images)\n",
    "predictions\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check to see the accuracy of model prediction\n",
    "# how well the model predicted an image? \n",
    "## BEGIN SOLUTION\n",
    "image_num = 20\n",
    "predict = class_names[np.argmax(predictions[image_num])]\n",
    "\n",
    "print(\"This Test Image \", image_num, predict)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(test_images[image_num].reshape(28,28), cmap = plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.9 Plotting the Predictor confidence\n",
    "Use the following function plot_image to plot the predictor confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code by Fran√ßois Chollet\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  \n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "  \n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1]) \n",
    "  predicted_label = np.argmax(predictions_array)\n",
    " \n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the plot_image to display images and how well they were predicted.\n",
    "## BEGIN SOLUTION\n",
    "i = 0\n",
    "plt.figure(figsize = (6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predictions, test_labels, test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predictions, test_labels)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "i = 12\n",
    "plt.figure(figsize = (6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predictions, test_labels, test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predictions, test_labels)\n",
    "plt.show()\n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.10\n",
    "Plot a 20 images (shown as 5x4 grid) to see test images and their confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first X test images, their predicted label, and the true label\n",
    "# Color correct predictions in blue, incorrect predictions in red\n",
    "## BEGIN SOLUTION\n",
    "rows = 5\n",
    "cols = 4\n",
    "imgs = rows * cols\n",
    "plt.figure(figsize=(2*2*cols, 2*rows))\n",
    "for i in range(imgs):\n",
    "    plt.subplot(rows, 2*cols, 2*i+1)\n",
    "    plot_image(i, predictions, test_labels, test_images)\n",
    "    plt.subplot(rows, 2*cols, 2*i+2)\n",
    "    plot_value_array(i, predictions, test_labels)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "Find the image with the least confidence above and re-train the network to see if you can improve the prediction accuracy to over 90%. You should not lose too much from the confidence in others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "Please provide feedback on this lab.\n",
    "* how would you rate this lab (from 1-10, 10-highest) : 8\n",
    "* how can we improve his lab? :N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2>Submission Instructions</h2> \n",
    "<b> File Name:</b> Please name the file as your_section_your_netID_finalProj.jpynb<br>\n",
    "<b> Submit To: </b> Canvas &rarr; Assignments &rarr; finalProj <br>\n",
    "<b>Warning:</b> Failure to follow directions may result in loss points.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab designed by A.D. Gunawardena, 2019. Acknowledgements: Google TensorFlow group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
