{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Student Information</h3> Please provide information about yourself.<br>\n",
    "<b>Name</b>: Veton Abazovic<br>\n",
    "<b>NetID</b>: va187<br>\n",
    "<b>Recitation (01/02)</b>: 02<br>\n",
    "<b>Notes to Grader</b> (optional):<br>\n",
    "<br><br>\n",
    "<b>IMPORTANT</b>\n",
    "Your work will not be graded withour your initials below<br>\n",
    "I certify that this lab represents my own work and I have read the RU academic intergrity policies at<br>\n",
    "<a href=\"https://www.cs.rutgers.edu/academic-integrity/introduction\">https://www.cs.rutgers.edu/academic-integrity/introduction </a><br>\n",
    "<b>Initials</b>:     VA\n",
    "\n",
    "\n",
    "<h3>Grader Notes</h3>\n",
    "<b>Your Grade<b>:<br>\n",
    "<b>Grader Initials</b>:<br>\n",
    "<b>Grader Comments</b> (optional):<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Linear Models, Feature Engineering and Logistic Regression\n",
    "\n",
    "** This lab is due Monday April 22, 2019 at 11:59pm (graded on accuracy and completeness) **\n",
    "\n",
    "In this lab we will work through the process of:\n",
    "1. implementing a linear model, defining loss functions, \n",
    "2. feature engineering,\n",
    "3. minimizing loss functions using numeric libraries \n",
    "4. how to implement logistic regression model for a binary classification problem\n",
    "\n",
    "We will use the toy tip calculation dataset from sns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Tips Dataset\n",
    "\n",
    "To begin with, we load the tips dataset from the `seaborn` library.  The tips data contains records of tips, total bill, and information about the person who paid the bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset(\"tips\")\n",
    "print(\"Number of Records:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Defining the Model and Feature Engineering\n",
    "\n",
    "In lab 6, we defined a simple linear model with only one parameter. Now let's make a more complicated model that utilizes other features in a dataset. Let our prediction for tip be a combination of the following features:\n",
    "\n",
    "$$\n",
    "\\text{Tip} = \\theta_1 * \\text{total_bill} + \\theta_2 * \\text{sex} + \\theta_3 * \\text{smoker} + \\theta_4 * \\text{day} + \\theta_5 * \\text{time} + \\theta_6 * \\text{size}\n",
    "$$\n",
    "\n",
    "Notice that some of these features are not numbers! But our linear model will need to predict a numerical value. \n",
    "### Task 1.1 Split the data into Y(tip) and X(all others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN SOLUTION\n",
    "tips = data['tip']\n",
    "X = data.drop(columns = 'tip')\n",
    "X\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Feature Engineering\n",
    "First, let's convert everything to numerical values. A straightforward approach is to map some of these non-numerical features into numerical ones. For example, we can treat the day as a value from 1-7. However, one of the disadvantages in directly translating to a numeric value is that we unintentially assign certain features disproportionate weight. Consider assigning Sunday to the numeric value of 7. Monday is assigned to 1 and thus Sunday has 7 times the influence of Monday in our linear model which can lower the accuracy of our model.\n",
    "\n",
    "solution: one-hot encoding! \n",
    "\n",
    "As discussed in lecture 9.2, one-hot encoding will produce a binary vector indicating the non-numeric feature. Sunday would be encoded as a [0 0 0 0 0 0 1]. This assigns a more even weight across each category in non-numeric features. Complete the code below to one-hot encode our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded dataframe of our input, data. \n",
    "    Hint: check pd.get_dummies\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return pd.get_dummies(data)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_X = one_hot_encode(X)\n",
    "one_hot_X = one_hot_X[['size', 'total_bill', 'day_Thur', 'day_Fri', 'day_Sat', 'day_Sun', 'sex_Male', 'sex_Female', 'smoker_Yes', 'smoker_No', 'time_Lunch', 'time_Dinner']]\n",
    "one_hot_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Defining the Model\n",
    "Now that all of our data is numeric, let's define our model function. Note that X and thetas are matrices now. Use matrix products to compute the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(thetas, X):\n",
    "    \"\"\"\n",
    "    Return the linear combination of thetas and features as defined above.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return X.dot(thetas)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_model(np.arange(1,5), np.arange(1,5)) == 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Fitting the Model: Numeric Methods\n",
    "Recall in the previous lab we defined multiple loss functions and found optimal theta using the scipy.minimize function. Adapt the loss functions and optimization code from the previous lab (provided below) to work with your new linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def abserror(y, y_hat):\n",
    "    return np.abs(y-y_hat)\n",
    "\n",
    "def sqerror(y, y_hat):\n",
    "    return (y - y_hat)**2\n",
    "\n",
    "def minimize_average_loss(loss_function, model, x, y):\n",
    "    \"\"\"\n",
    "    loss_function: either the squared or absolute loss functions from above.\n",
    "    model: the model (as defined above)\n",
    "    x: the x values (one-hot encoded data)\n",
    "    y: the y values (tip amounts)\n",
    "    return the estimate for each theta as a vector\n",
    "    \n",
    "    Note we will ignore failed convergence for this lab ... \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Notes on the following function call which you need to finish:\n",
    "    # \n",
    "    # 0. the ... should be replaced with the average loss evaluated on \n",
    "    #       the data x, y using the model and appropriate loss function\n",
    "    # 1. x0 are the initial values for THETA.  Yes, this is confusing\n",
    "    #       but optimization people like x to be the thing they are \n",
    "    #       optimizing.\n",
    "    # 2. We extract the 'x' entry in the dictionary which corresponds\n",
    "    #       to the value of thetas at the optimum\n",
    "    ### BEGIN SOLUTION\n",
    "    return minimize(lambda theta: loss_function(model(theta, x), y).mean(), x0=np.random.rand(x.shape[1]))['x']\n",
    "    ### END SOLUTION\n",
    "    #return minimize(lambda theta: ..., x0=...)\n",
    "minimize_average_loss(sqerror, linear_model, one_hot_X, tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Fitting the Model: Analytic Methods\n",
    "Let's also fit our model analytically, for the l2 loss function. In this question we will derive an analytical solution, fit our model and compare our results with our numerical optimization results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Least Squares Solution\n",
    "Recall that if we're fitting a linear model with the l2 loss function, we are performing least squares! Remember, we are solving the following optimization problem for least squares:\n",
    "\n",
    "$$\\min_{\\theta} ||X\\theta - y||^2$$\n",
    "\n",
    "Let's begin by deriving the analytic solution to least squares. Write your answer in LaTeX in the cell below. Assume X is full column rank. You are given step 1 to get things started. You need to complete the steps 2-4 where step 4 shows the value of theta that optimizes given X and y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "Take the gradient and set theta equal to 0. \n",
    "\n",
    "$$2(X\\theta - y) = 0$$\n",
    "$$ step 2: (X\\theta - y)^T(X\\theta -y)$$\n",
    "$$(\\theta^TX^T - y^T)(X\\theta-y) $$\n",
    "$$ \\theta^T X^T X\\theta - \\theta^T X^T y -y^T X\\theta +y^T y$$\n",
    "$$ \\theta^T X^T X\\theta -2\\theta^T X^T y + y^T y $$\n",
    "$$ step 3 - Derivatives:$$\n",
    "$$ d/theta(\\theta^T X^T X\\theta) - d/theta(2\\theta^T X^T) + d/theta(y^T y) = 0$$\n",
    "$$ (X^T X +( X^T X)^T)\\theta - 2X^T y + 0 = 0$$\n",
    "$$ 2X^T X\\theta = 2X^Ty$$\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Solving for Theta\n",
    "Using your formuala above, let us find the analytic solution for $\\theta$. That is, the optimal numerical thetas for our tips dataset. Fill out the function below. Make sure you use the float type in your calculations using .astype(float) and use the np.linalg.inv function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analytical(x, y):\n",
    "    \"\"\"\n",
    "    x: our one-hot encoded dataset\n",
    "    y: tip amounts\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    xTx = x.T.dot(x)\n",
    "    xTy = x.T.dot(y)\n",
    "    return np.linalg.inv(xTx).dot(xTy)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_thetas = get_analytical(one_hot_X.astype(float), tips.astype(float))\n",
    "print(\"Our analytical loss is: \", sqerror(linear_model(analytical_thetas, one_hot_X),tips).mean())\n",
    "print(\"Our numerical loss is: \", sqerror(linear_model(minimize_average_loss(sqerror, linear_model, one_hot_X, tips), one_hot_X), tips).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(sqerror(linear_model(analytical_thetas, one_hot_X),tips).mean(), 59.733414754098362)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Weird Results?\n",
    "Our analytical loss is surprisingly much worse than our numerical loss. Why is this? \n",
    "\n",
    "Hint: https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li\n",
    "\n",
    "Answer: The reason our analytical loss is surprisingly much worse than our numerical loss is because when you use sum the numbers are close together and when you use the other method the numbers are further part. Using np.linalg.inv loses precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Logistic Regression\n",
    "In this part we will implement a logistic regression model with the tip data set. We will use Male and Female as the binary classification. We will use the LogisiticRegression model from sklearn to find a prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 4.1\n",
    "** create the X (one-hot features) and y (sex) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_hot_encode(X)\n",
    "y = data['sex']\n",
    "X = X[['total_bill','size','smoker_Yes','smoker_No','day_Thur','day_Fri','day_Sat', 'day_Sun','time_Lunch', 'time_Dinner']]\n",
    "## END SOLUTION\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 4.2 \n",
    "Split the data set to training and test using sklearn train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## BEGIN SOLUTION\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "## END SOLUTION\n",
    "\n",
    "print (X_train.head(), X_test.head(), y_train.head(), y_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 4.3 logistic regression model\n",
    "Fit a logisitic regression model using LogisticRegression from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "## BEGIN SOLUTION\n",
    "logmodel = LogisticRegression().fit(X_train, y_train)\n",
    "print(logmodel)\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 4.4 Predicting\n",
    "Now we can use the logmodel from previous part to predict on the test data. Read documentation to see how. It should be just a one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN SOLUTION\n",
    "predictions = logmodel.predict(X_test)\n",
    "#predictions\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 4.5 reporting\n",
    "use classification_report from sklearn.metrics to report the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "## BEGIN SOLUTION\n",
    "target_names = ['Female', 'Male']\n",
    "print(classification_report(y_test, predictions, target_names=target_names))\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "Please provide feedback on this lab.\n",
    "* how would you rate this lab (from 1-10, 10-highest) : 9\n",
    "* how can we improve his lab? :n/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2>Submission Instructions</h2> \n",
    "<b> File Name:</b> Please name the file as your_section_your_netID_lab7.jpynb<br>\n",
    "<b> Submit To: </b> Canvas &rarr; Assignments &rarr; lab7 <br>\n",
    "<b>Warning:</b> Failure to follow directions may result in loss points.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developed by A.D. Gunawardena. Credits: Josh Hug, Berkeley Data Science Lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
